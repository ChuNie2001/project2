---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Chu Nie cn9863

### Introduction 

The data set contains the data we need for a simple customer personality behavior analysis. The dataset contains 29 variables and 2216 observations in total (after dropping the NAs). After cleaning the dataset, there are 25 variables.The main variable that I use for the project is Recency,Year_Birth, Income, Res, and several variables that present the number of times that the customer made purchasements. Recency is the number of days since customer's last purchase. The Year_Birth is the customer's birth year. Income gives us the customer's yearly household income. If Res equals to 1, the customer accepted the offer in the campaign, 0 otherwise.(The binary variable) Response is the number of times that a customer respond to the offer.NumWebPurchases: Number of purchases made through the company’s website. NumCatalogPurchases: Number of purchases made using a catalogue.NumStorePurchases: Number of purchases made directly in stores.NumWebVisitsMonth: Number of visits to company’s website in the last month.

```{R}
library(tidyverse)
library(readr)
library(dplyr)
# read your datasets in here, e.g., with read_csv()
data <- read.csv("marketing_campaign.csv",sep = "\t")

data <- data %>% na.omit()

data <- data %>% mutate(Response = Response + AcceptedCmp1+AcceptedCmp2+AcceptedCmp3+AcceptedCmp4+AcceptedCmp5) %>% mutate(Res = if_else(Response != 0, 1,0)) %>% select(-c(AcceptedCmp1,AcceptedCmp2,AcceptedCmp3,AcceptedCmp4,AcceptedCmp5))
```

### Cluster Analysis

```{R}
library(cluster)
library(GGally)
# clustering code here
pam_dat<-data%>%select(Recency, NumDealsPurchases, NumWebPurchases, 
    NumStorePurchases)%>% scale

sil_width<-vector()
for(i in 2:10){
  pam_fit <- pam(pam_dat, k = i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)


pam1 <- pam_dat %>% pam(k=2)
pam1

pamclust<-data %>% mutate(cluster=as.factor(pam1$clustering)) 
pamclust
pamclust%>% ggpairs(col = c("Recency", "NumDealsPurchases", 
    "NumWebPurchases", "NumStorePurchases") ,aes(color = cluster))

```

Discussion of clustering here
    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
library(ggplot2)
d1 <- data %>% select(Recency,Kidhome,Teenhome,MntWines,MntFruits,MntMeatProducts,MntFishProducts,MntSweetProducts,MntGoldProds,NumDealsPurchases,NumWebPurchases,NumStorePurchases,NumCatalogPurchases,NumWebVisitsMonth)%>% scale
pca_1 <- princomp(d1,cor = T)
summary(pca_1,loadings = T)

data %>% mutate(PC1=pca_1$scores[, 1], PC2=pca_1$scores[, 2]) %>%
  ggplot(aes(PC1, PC2)) + geom_point(aes(color = data$Response)) + coord_fixed()
```

Discussions of PCA here. 

###  Linear Classifier

```{R}
# linear classifier code here
fit <- glm(Res ~ Income+Kidhome+Teenhome+Recency+NumDealsPurchases+NumWebPurchases+NumCatalogPurchases+NumWebVisitsMonth+Complain+Year_Birth, data=data, family="binomial")

score <- predict(fit,type="response")
#score %>% round(3)

class_diag <- function(score, truth, positive, cutoff=.5, strictlygreater=T){
  if(strictlygreater==T) pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  else pred <- factor(score>=cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]
#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}

class_diag(score,data$Res,positive = 1)

table(truth= factor(data$Res==1, levels=c("TRUE","FALSE")),
      prediction= factor(score>.5, levels=c("TRUE","FALSE")))


```

```{R}
# cross-validation of linear classifier here
set.seed(1234)
k=10 
temp<-data[sample(nrow(data)),] 
folds<-cut(seq(1:nrow(temp)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
  train<-temp[folds!=i,]
  test<-temp[folds==i,]
  truth<-test$Res
  fit<-glm(Res~Income+Kidhome+Teenhome+Recency+NumDealsPurchases+NumWebPurchases+NumCatalogPurchases+NumWebVisitsMonth+Complain+Year_Birth,data=train,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}

summarize_all(diags,mean)
```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(factor(Res==1,levels=c("TRUE","FALSE")) ~ Income+Kidhome+Teenhome+Recency+NumDealsPurchases+NumWebPurchases+NumCatalogPurchases+NumWebVisitsMonth+Complain+Year_Birth, data=data, k=5)

knn1 <- predict(knn_fit,data)
knn1

class_diag(knn1[,1],data$Res, positive=1)

table(truth= factor(data$Res==1, levels=c("TRUE","FALSE")),
      prediction= factor(knn1[,1]>.5, levels=c("TRUE","FALSE")))


```

```{R}
# cross-validation of np classifier here
set.seed(1234)
k=10 
temp<-data[sample(nrow(data)),] 
folds<-cut(seq(1:nrow(temp)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
  train<-temp[folds!=i,]
  test<-temp[folds==i,]
  truth<-test$Res
  fit<-knn3(Res~Income+Kidhome+Teenhome+Recency+NumDealsPurchases+NumWebPurchases+NumCatalogPurchases+NumWebVisitsMonth+Complain+Year_Birth,data=train)
  probs<-predict(fit,newdata = test)[,2]
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}

summarize_all(diags,mean)
```

Discussion


### Regression/Numeric Prediction

```{R}
# regression model code here
data1 <- data %>% select(-Dt_Customer)
reg1 <- lm(Res~.,data = data1)

reg1_sum <- summary(reg1)

mse <- mean(reg1_sum$residuals^2)

mse

```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5 
data1 <- data %>% select(-Dt_Customer)
temp<-data1[sample(nrow(data1)),]
folds<-cut(seq(1:nrow(data1)),breaks=k,labels=F) 
diags<-NULL
for(i in 1:k){
  train<-temp[folds!=i,]
  test<-temp[folds==i,]
  fit<-lm(Res~.,data=train)
  yhat<-predict(fit,newdata=test)
  diags<-mean((test$Res-yhat)^2)
}
mean(diags)

```

Discussion

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required = F)
income <- data$Income
```

```{python}
# python code here
a = max(r.income)
b = min(r.income)
print(min(r.income),max(r.income))
print(max(r.income)-min(r.income))

```
```{R}
py$b
py$a
py$a - py$b
```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




